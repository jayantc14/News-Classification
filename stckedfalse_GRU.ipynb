{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fake = pd.read_excel(\"D:\\IIM Intern\\LIWC2015 Results (Fake).xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "fake['Target'] = 'fake'\n",
    "\n",
    "true = pd.read_excel(\"D:\\IIM Intern\\LIWC2015 Results (True).xlsx\")\n",
    "\n",
    "true['Target'] = 'true'\n",
    "\n",
    "df = pd.concat([true,fake])\n",
    "\n",
    "df.reset_index(inplace = True,drop = True)\n",
    "\n",
    "\n",
    "df['text']=df['text']+\" \"+df['title']\n",
    "df.drop(['subject','date','title'],axis = 1,inplace = True)\n",
    "\n",
    "df.columns\n",
    "\n",
    "df.drop(columns = ['WC','Clout','Dic','discrep','affiliation'],inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "dfff = df['text'].to_numpy(copy = False)\n",
    "\n",
    "df['FREScore'] = 0\n",
    "\n",
    "n = len(dfff)\n",
    "for epoch in range(n):\n",
    "  t = textstat.flesch_reading_ease(dfff[epoch])\n",
    "  df.loc[epoch,'FREScore']  = t\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Target', axis=1)\n",
    "\n",
    "y = df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Encode labels in column 'species'.\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = open('D:\\IIM Intern\\glove.6B.100d.txt', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Authentic</th>\n",
       "      <th>Tone</th>\n",
       "      <th>WPS</th>\n",
       "      <th>Sixltr</th>\n",
       "      <th>number</th>\n",
       "      <th>quant</th>\n",
       "      <th>posemo</th>\n",
       "      <th>negemo</th>\n",
       "      <th>...</th>\n",
       "      <th>certain</th>\n",
       "      <th>achieve</th>\n",
       "      <th>power</th>\n",
       "      <th>reward</th>\n",
       "      <th>risk</th>\n",
       "      <th>focuspast</th>\n",
       "      <th>focuspresent</th>\n",
       "      <th>focusfuture</th>\n",
       "      <th>Target</th>\n",
       "      <th>FREScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>97.02</td>\n",
       "      <td>22.27</td>\n",
       "      <td>35.07</td>\n",
       "      <td>23.41</td>\n",
       "      <td>28.44</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.20</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.93</td>\n",
       "      <td>4.01</td>\n",
       "      <td>6.81</td>\n",
       "      <td>2.00</td>\n",
       "      <td>true</td>\n",
       "      <td>42.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>96.86</td>\n",
       "      <td>13.98</td>\n",
       "      <td>49.52</td>\n",
       "      <td>20.23</td>\n",
       "      <td>32.85</td>\n",
       "      <td>2.39</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1.44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.96</td>\n",
       "      <td>8.61</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.12</td>\n",
       "      <td>3.83</td>\n",
       "      <td>5.10</td>\n",
       "      <td>2.07</td>\n",
       "      <td>true</td>\n",
       "      <td>32.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>94.73</td>\n",
       "      <td>16.26</td>\n",
       "      <td>29.43</td>\n",
       "      <td>24.05</td>\n",
       "      <td>27.35</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>6.13</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.47</td>\n",
       "      <td>5.47</td>\n",
       "      <td>1.97</td>\n",
       "      <td>true</td>\n",
       "      <td>39.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>96.40</td>\n",
       "      <td>12.12</td>\n",
       "      <td>55.62</td>\n",
       "      <td>22.29</td>\n",
       "      <td>32.19</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.32</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.33</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.53</td>\n",
       "      <td>true</td>\n",
       "      <td>45.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>96.29</td>\n",
       "      <td>22.73</td>\n",
       "      <td>15.88</td>\n",
       "      <td>22.05</td>\n",
       "      <td>27.21</td>\n",
       "      <td>4.19</td>\n",
       "      <td>3.14</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.86</td>\n",
       "      <td>...</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.93</td>\n",
       "      <td>3.49</td>\n",
       "      <td>5.47</td>\n",
       "      <td>0.23</td>\n",
       "      <td>true</td>\n",
       "      <td>50.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44873</td>\n",
       "      <td>21st Century Wire says As 21WIRE reported earl...</td>\n",
       "      <td>96.34</td>\n",
       "      <td>16.74</td>\n",
       "      <td>46.26</td>\n",
       "      <td>49.00</td>\n",
       "      <td>28.01</td>\n",
       "      <td>1.11</td>\n",
       "      <td>2.04</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.78</td>\n",
       "      <td>...</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.74</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.37</td>\n",
       "      <td>5.19</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.37</td>\n",
       "      <td>fake</td>\n",
       "      <td>22.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44874</td>\n",
       "      <td>21st Century Wire says It s a familiar theme. ...</td>\n",
       "      <td>94.03</td>\n",
       "      <td>14.89</td>\n",
       "      <td>56.75</td>\n",
       "      <td>27.73</td>\n",
       "      <td>19.02</td>\n",
       "      <td>3.28</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.30</td>\n",
       "      <td>5.57</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3.93</td>\n",
       "      <td>6.23</td>\n",
       "      <td>0.98</td>\n",
       "      <td>fake</td>\n",
       "      <td>51.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44875</td>\n",
       "      <td>Patrick Henningsen  21st Century WireRemember ...</td>\n",
       "      <td>90.88</td>\n",
       "      <td>33.24</td>\n",
       "      <td>22.15</td>\n",
       "      <td>38.88</td>\n",
       "      <td>22.66</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.71</td>\n",
       "      <td>...</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.68</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.83</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1.52</td>\n",
       "      <td>fake</td>\n",
       "      <td>28.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44876</td>\n",
       "      <td>21st Century Wire says Al Jazeera America will...</td>\n",
       "      <td>91.99</td>\n",
       "      <td>25.14</td>\n",
       "      <td>49.16</td>\n",
       "      <td>47.70</td>\n",
       "      <td>20.34</td>\n",
       "      <td>4.40</td>\n",
       "      <td>2.31</td>\n",
       "      <td>2.52</td>\n",
       "      <td>1.26</td>\n",
       "      <td>...</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.10</td>\n",
       "      <td>3.14</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.77</td>\n",
       "      <td>7.13</td>\n",
       "      <td>0.84</td>\n",
       "      <td>fake</td>\n",
       "      <td>27.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44877</td>\n",
       "      <td>21st Century Wire says As 21WIRE predicted in ...</td>\n",
       "      <td>95.49</td>\n",
       "      <td>21.45</td>\n",
       "      <td>10.28</td>\n",
       "      <td>35.68</td>\n",
       "      <td>23.54</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.23</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.79</td>\n",
       "      <td>4.48</td>\n",
       "      <td>6.61</td>\n",
       "      <td>1.46</td>\n",
       "      <td>fake</td>\n",
       "      <td>48.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44878 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  Analytic  Authentic  \\\n",
       "0      WASHINGTON (Reuters) - The head of a conservat...     97.02      22.27   \n",
       "1      WASHINGTON (Reuters) - Transgender people will...     96.86      13.98   \n",
       "2      WASHINGTON (Reuters) - The special counsel inv...     94.73      16.26   \n",
       "3      WASHINGTON (Reuters) - Trump campaign adviser ...     96.40      12.12   \n",
       "4      SEATTLE/WASHINGTON (Reuters) - President Donal...     96.29      22.73   \n",
       "...                                                  ...       ...        ...   \n",
       "44873  21st Century Wire says As 21WIRE reported earl...     96.34      16.74   \n",
       "44874  21st Century Wire says It s a familiar theme. ...     94.03      14.89   \n",
       "44875  Patrick Henningsen  21st Century WireRemember ...     90.88      33.24   \n",
       "44876  21st Century Wire says Al Jazeera America will...     91.99      25.14   \n",
       "44877  21st Century Wire says As 21WIRE predicted in ...     95.49      21.45   \n",
       "\n",
       "        Tone    WPS  Sixltr  number  quant  posemo  negemo  ...  certain  \\\n",
       "0      35.07  23.41   28.44    2.80   1.34    1.60    1.07  ...     0.27   \n",
       "1      49.52  20.23   32.85    2.39   1.28    2.71    1.44  ...     0.48   \n",
       "2      29.43  24.05   27.35    0.88   1.75    1.09    0.88  ...     0.66   \n",
       "3      55.62  22.29   32.19    2.11   0.53    2.11    0.53  ...     0.26   \n",
       "4      15.88  22.05   27.21    4.19   3.14    1.16    1.86  ...     0.81   \n",
       "...      ...    ...     ...     ...    ...     ...     ...  ...      ...   \n",
       "44873  46.26  49.00   28.01    1.11   2.04    3.90    2.78  ...     0.93   \n",
       "44874  56.75  27.73   19.02    3.28   1.97    2.95    1.31  ...     0.33   \n",
       "44875  22.15  38.88   22.66    1.59   1.99    2.48    2.71  ...     1.54   \n",
       "44876  49.16  47.70   20.34    4.40   2.31    2.52    1.26  ...     2.10   \n",
       "44877  10.28  35.68   23.54    1.23   1.35    1.01    2.24  ...     0.78   \n",
       "\n",
       "       achieve  power  reward  risk  focuspast  focuspresent  focusfuture  \\\n",
       "0         1.20   5.87    0.80  0.93       4.01          6.81         2.00   \n",
       "1         0.96   8.61    0.80  1.12       3.83          5.10         2.07   \n",
       "2         0.66   6.13    0.44  0.00       5.47          5.47         1.97   \n",
       "3         1.32   6.60    0.53  0.00       6.33          3.43         0.53   \n",
       "4         0.81   3.60    0.70  0.93       3.49          5.47         0.23   \n",
       "...        ...    ...     ...   ...        ...           ...          ...   \n",
       "44873     0.74   3.15    0.93  0.37       5.19          3.71         0.37   \n",
       "44874     2.30   5.57    2.30  0.33       3.93          6.23         0.98   \n",
       "44875     1.68   4.68    0.77  1.03       2.83          5.94         1.52   \n",
       "44876     2.10   3.14    1.68  1.05       3.77          7.13         0.84   \n",
       "44877     1.23   4.26    0.78  1.79       4.48          6.61         1.46   \n",
       "\n",
       "       Target FREScore  \n",
       "0        true    42.04  \n",
       "1        true    32.57  \n",
       "2        true    39.91  \n",
       "3        true    45.49  \n",
       "4        true    50.80  \n",
       "...       ...      ...  \n",
       "44873    fake    22.35  \n",
       "44874    fake    51.41  \n",
       "44875    fake    28.24  \n",
       "44876    fake    27.16  \n",
       "44877    fake    48.88  \n",
       "\n",
       "[44878 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM, RNN\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train = []\n",
    "sentences = list(X_train[\"text\"])\n",
    "for sen in sentences:\n",
    "    X1_train.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test = []\n",
    "sentences = list(X_test[\"text\"])\n",
    "for sen in sentences:\n",
    "    X1_test.append(preprocess_text(sen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X1_train)\n",
    "\n",
    "X1_train = tokenizer.texts_to_sequences(X1_train)\n",
    "X1_test = tokenizer.texts_to_sequences(X1_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "X1_train = pad_sequences(X1_train, padding='post', maxlen=maxlen)\n",
    "X1_test = pad_sequences(X1_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train = X_train[['Analytic', 'Authentic', 'Tone', 'WPS', 'Sixltr', 'number',\n",
    "       'quant', 'posemo', 'negemo', 'tentat', 'certain', 'achieve', 'power',\n",
    "       'reward', 'risk', 'focuspast', 'focuspresent', 'focusfuture',\n",
    "       'FREScore']].values\n",
    "X2_test = X_test[['Analytic', 'Authentic', 'Tone', 'WPS', 'Sixltr', 'number',\n",
    "       'quant', 'posemo', 'negemo', 'tentat', 'certain', 'achieve', 'power',\n",
    "       'reward', 'risk', 'focuspast', 'focuspresent', 'focusfuture',\n",
    "       'FREScore']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(maxlen,))\n",
    "\n",
    "input_2 = Input(shape=(19,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(input_1)\n",
    "\n",
    "\n",
    "LSTM_Layer_1 = GRU(128)(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer_1 = Dense(10, activation='relu')(input_2)\n",
    "dense_layer_2 = Dense(10, activation='relu')(dense_layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concat_layer = Concatenate()([LSTM_Layer_1, dense_layer_2])\n",
    "dense_layer_3 = Dense(10, activation='relu')(concat_layer)\n",
    "output = Dense(2, activation='softmax')(dense_layer_3)\n",
    "model = Model(inputs=[input_1, input_2], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 19)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 100)     10564800    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           200         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (None, 128)          88320       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           110         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 138)          0           gru[0][0]                        \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           1390        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            22          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 10,654,842\n",
      "Trainable params: 90,042\n",
      "Non-trainable params: 10,564,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "225/225 [==============================] - 131s 580ms/step - loss: 0.3818 - acc: 0.8508 - val_loss: 0.1078 - val_acc: 0.9667\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - 115s 511ms/step - loss: 0.1155 - acc: 0.9594 - val_loss: 0.1650 - val_acc: 0.9302\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - 118s 523ms/step - loss: 0.0822 - acc: 0.9711 - val_loss: 0.0801 - val_acc: 0.9709\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - 120s 531ms/step - loss: 0.0630 - acc: 0.9779 - val_loss: 0.0474 - val_acc: 0.9848\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - 121s 539ms/step - loss: 0.0360 - acc: 0.9885 - val_loss: 0.0358 - val_acc: 0.9883\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - 119s 531ms/step - loss: 0.0286 - acc: 0.9914 - val_loss: 0.0253 - val_acc: 0.9925\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - 51s 227ms/step - loss: 0.0176 - acc: 0.9948 - val_loss: 0.0225 - val_acc: 0.9932\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - 54s 238ms/step - loss: 0.0242 - acc: 0.9923 - val_loss: 0.4523 - val_acc: 0.7972\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - 48s 215ms/step - loss: 0.0578 - acc: 0.9778 - val_loss: 0.0246 - val_acc: 0.9903\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - 52s 230ms/step - loss: 0.0144 - acc: 0.9951 - val_loss: 0.0189 - val_acc: 0.9928\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[X1_train, X2_train], y=y_train, batch_size=128, epochs=10, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 7s 24ms/step - loss: 0.0207 - acc: 0.9929\n",
      "Test Score: 0.02067633531987667\n",
      "Test Accuracy: 0.9928698539733887\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model.predict(x=[X1_test, X2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (model.predict(x=[X1_test, X2_test])>=0.5).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4651\n",
      "           1       0.99      0.99      0.99      4325\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      8976\n",
      "   macro avg       0.99      0.99      0.99      8976\n",
      "weighted avg       0.99      0.99      0.99      8976\n",
      " samples avg       0.99      0.99      0.99      8976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
